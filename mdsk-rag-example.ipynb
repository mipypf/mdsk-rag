{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=HF_TOKEN) # Replace HF_TOKEN with your actual Hugging Face token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    transformers.set_seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(YOUR_CSV_FILE_PATH) # Replace with your actual CSV file path\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'] + ' ' + df['labels']\n",
    "df = df[['text']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-2-9b-it\" # Change to other LLMs (e.g. \"google/gemma-2-27b-it\", \"Qwen/Qwen2.5-7B-Instruct\") as needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path, max_tokens=150):\n",
    "    text = \"\"\n",
    "    chunks = []\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    \n",
    "    tokens = tokenizer(text, return_tensors=\"np\", truncation=False)[\"input_ids\"][0]\n",
    "    for i in range(0, len(tokens), max_tokens):\n",
    "        chunk_tokens = tokens[i:i+max_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_embeddings = embedding_model.encode(df['text'].values, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_paths = glob(YOUR_PDF_FILE_PATH) # Replace with your actual PDF file paths\n",
    "pdf_texts = []\n",
    "for pdf_path in pdf_paths:\n",
    "    pdf_texts.extend(extract_text_from_pdf(pdf_path, max_tokens=150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_embeddings = embedding_model.encode(pdf_texts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_index = faiss.IndexFlatL2(csv_embeddings.shape[1])\n",
    "csv_index.add(csv_embeddings.astype('float32'))\n",
    "\n",
    "pdf_index = faiss.IndexFlatL2(pdf_embeddings.shape[1])\n",
    "pdf_index.add(pdf_embeddings.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, temperature=0,\n",
    "                            #  , do_sample=False  Add this line when an error occurs (e.g. using Qwen2.5-7B-Instruct)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, pipeline, max_new_tokens=512):\n",
    "    prompt = (\n",
    "    \"YOUR_SUMMARY_PROMPT:\\n\\n\" # This should be defined in your context\n",
    "    f\"{text}\\n\\nSummary:\")\n",
    "    outputs = pipeline(prompt, max_new_tokens=max_new_tokens)\n",
    "    generated_text = outputs[0]['generated_text'].strip()\n",
    "\n",
    "    if \"Summary:\" in generated_text:\n",
    "        summary = generated_text.split(\"Summary:\")[-1].strip()\n",
    "    else:\n",
    "        summary = generated_text\n",
    "\n",
    "    return f\"\\n{summary}\\n\"\n",
    "\n",
    "def ask_question(question, embedding_model, csv_index, pdf_index, csv_data, pdf_data, text_gen_pipeline, max_new_tokens=512, k=10):\n",
    "    question_embedding = embedding_model.encode([question], convert_to_tensor=False)\n",
    "    \n",
    "    csv_distances, csv_indices = csv_index.search(np.array(question_embedding), k=k)\n",
    "    pdf_distances, pdf_indices = pdf_index.search(np.array(question_embedding), k=k)\n",
    "    \n",
    "    csv_texts = []\n",
    "    pdf_texts = []\n",
    "    \n",
    "    for idx in csv_indices[0]:\n",
    "        csv_text = csv_data.iloc[idx]['text']\n",
    "        csv_texts.append(csv_text)\n",
    "        print(f\"CSV Hit:\\n{csv_text}\\n\")\n",
    "    \n",
    "    for idx in pdf_indices[0]:\n",
    "        pdf_text = pdf_data[idx]\n",
    "        pdf_texts.append(pdf_text)\n",
    "        print(f\"PDF Hit:\\n{pdf_text}\\n\")\n",
    "    \n",
    "    retrieved_texts = []\n",
    "\n",
    "    if csv_texts:\n",
    "        combined_csv_text = \"\\n\".join(csv_texts)\n",
    "        csv_summary = summarize_text(combined_csv_text, text_gen_pipeline, max_new_tokens=512)\n",
    "        print(f\"CSV Summary:\\n{csv_summary}\\n\")\n",
    "        retrieved_texts.insert(0, f\"From experimental data: {csv_summary}\")\n",
    "    \n",
    "    if pdf_texts:\n",
    "        combined_pdf_text = \"\\n\".join(pdf_texts)\n",
    "        pdf_summary = summarize_text(combined_pdf_text, text_gen_pipeline, max_new_tokens=512)\n",
    "        print(f\"PDF Summary:\\n{pdf_summary}\\n\")\n",
    "        retrieved_texts.append(f\"From paper findings: {pdf_summary}\")\n",
    "    \n",
    "    retrieved_context = \"\\n\".join(retrieved_texts)\n",
    "    print(f\"Related Information:\\n{retrieved_context}\\n\")\n",
    "    input_text = f\"Question: {question}\\n\\nRelated Information: \\n\\n{retrieved_context}\\n\\nAnswer: \"\n",
    "    \n",
    "    outputs = text_gen_pipeline(input_text, max_new_tokens=max_new_tokens)\n",
    "    assistant_response = outputs[0]['generated_text'].strip()\n",
    "\n",
    "    return assistant_response, csv_texts, pdf_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = YOUR_QUESTION # Replace with your actual question\n",
    "outputs = text_gen_pipeline(question, max_new_tokens=512)\n",
    "response = outputs[0]['generated_text'].strip()\n",
    "print(response) # Without MDSK-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response)) # Without MDSK-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, related_csv, related_pdf = ask_question(question, embedding_model, csv_index, pdf_index, df, pdf_texts, text_gen_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer) # With MDSK-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(answer)) # With MDSK-RAG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
